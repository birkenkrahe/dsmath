#+TITLE: MEASURES OF SPREAD AND CORRELATION
#+AUTHOR: MARCUS BIRKENKRAHE
#+SUBTITLE: Applied math for data science (DSC 482/MTH 445) Fall 2022
#+PROPERTY: :session *R* :results output :exports both
#+STARTUP: overview hideblocks indent inlineimages entitiespretty
#+attr_html: :width 700px
#+caption: John Wilder Tukey (1915-2000)
[[../img/tukey.jpg]]

1. [X] Quantiles, percentiles, and the five-number-summary
2. [X] Measures of spread: variance, standard deviation and IQR
3. [ ] Covariance and correlation
4. [ ] Outliers

#+STARTUP:overview hideblocks indent inlineimages
#+PROPERTY: header-args:R :export both :results output :session *R*
* Prequel: why distributions?

- Statistics are derived from experimental observations, which form
  values of variables stored in column vectors of data frames

- As soon as you observe 1 value, you have a /distribution/. The more
  values, the more interesting the distribution (for classification)

- Example: a "normal" distribution (also Gaussian or Bell curve)
  #+caption: Bell curve with sample mean and standard deviation
  [[../img/5_normal.png]]

- This can be reproduced in R with pseudo-random numbers from ~rnorm~.
  #+begin_src R :session *R* :exports both :results output graphics file :file ../img/5_binom.png
    par(mfrow=c(2,1))
    hist(rnorm(1e+05))
    plot(density(rnorm(1e+05)))
  #+end_src

  #+RESULTS:
  [[file:../img/5_binom.png]]

  The distribution as a pattern exhibits measures of centrality (like
  \mu), and measures of spread (like \sigma).

* Quantiles, percentiles and five-number-summary

- A /quantile/ is a value that characterizes a distribution: it
  characterizes the rank of an observation

- The /median/ or middle magnitude is the 0.5th quantile: half of all
  measurements lie below (and above) it
  #+begin_src R :session *R* :exports both :results output graphics file :file ../img/5_median.png
    plot(density(chickwts$weight))
    abline(v=median(chickwts$weight), col="red")
    abline(v=mean(chickwts$weight), col="blue")
  #+end_src

  #+RESULTS:
  [[file:../img/5_median.png]]

- Quantiles can be expressed as /percentiles/, on a scale of [0,100]
  instead of [0,1]: the p-th quantile is the 100*p-th percentile

- All algorithms to identify quantiles/percentiles sort observations,
  and then compute a weighted average to find p numerically

* Quantiles by hand

Do it by hand! What is the 0th / 0.25th / 0.5th / .75th and 1th
quantile of ~foo <- c(100, 498.5, 22, 0, -33, 100.66, -2)~
#+begin_src R
  foo <- c(100, 498.5, 22, 0, -33, 100.66, -2)
  foo
#+end_src
1) Which number is smaller than any other (0th quantile/percentile -
   0 numbers lie below it). *Minimum of the data*.
   #+begin_src R
     min(foo)
   #+end_src
2) The 25th percentile splits off the lowest 25% from the highest
   75%. This number is the middle number between the smallest number
   (minimum, 0th quantile) and the median (0.5th quantile), or the
   *median of the lower half of the data*.
   #+begin_src R
     foo_s <- sort(foo)  # sort vector from smallest to largest
     foo_s
     median(foo_s) # compute median: (-2+1)/2 = -1
     median(c(-33,-2,0,22)) # 0.25th quartile
   #+end_src
3) The 0.5th quartile or 50th percentile: half of the values are
   smaller than it - *median of the data*.
   #+begin_src R
     median(foo)
   #+end_src
4) The 0.75th quartile or 75% percentile: 3/4 of all values are
   smaller than it. This number is the middle number between the
   largest number (maximum), or the *median of the upper half of the
   data*.
   #+begin_src R
     foo_s <- sort(foo)  # sort vector from smallest to largest
     foo_s
     median(foo_s) # compute median: (-2+1)/2 = -1
     median(c(22,100,100.66,498.50)) # 75th percentile
   #+end_src
5) The 1.00th quartile or 100% percentile: all values are smaller than
   it. This number is the *maximum value of the data*.
   #+begin_src R
     max(foo)
   #+end_src
   Compare the result with the ~quantile~ algorithm:
   #+begin_src R
     q <- quantile(foo)
     names(q)<-NULL
     identical(q,c(-33,-1,22,100.33,498.5))
   #+end_src
* Quantiles with ~quantile~

- Using our old friend ~xdata <- c(2,4.4,3,3,2,2.2,2,4)~, we can use
  ~quantile~ to compute the 0th to 4th quantile.
  #+begin_src R
    xdata <- c(2,4.4,3,3,2,2.2,2,4)
    xdata
    sort(xdata)
    quantile(xdata)
  #+end_src

  #+RESULTS:
  : [1] 2.0 4.4 3.0 3.0 2.0 2.2 2.0 4.0
  : [1] 2.0 2.0 2.0 2.2 3.0 3.0 4.0 4.4
  :   0%  25%  50%  75% 100%
  : 2.00 2.00 2.60 3.25 4.40

- With ~quantile~, we can also compute other quantiles, like the 0.8th
  quantile (or 80th percentile): 80% of all values are smaller than it:
  #+begin_src R
    quantile(xdata, prob=0.8)
  #+end_src

  #+RESULTS:
  : 80%
  : 3.6

- [ ] Does ~quantile~ allow removing ~NA~ values?
  #+begin_src R
    quantile(c(xdata, NA), prob=0.8, na.rm=TRUE)
  #+end_src
- ~quantile~ is a generic function and can take multiple input formats
  #+begin_src R
    methods(quantile)
  #+end_src
- ~quantile~ can also handle probability vectors.
  #+begin_src R
    quantile(xdata, prob=c(0, .25, 0.75, 1))
  #+end_src
- [ ] What happens if you choose ~prob > 1~
  #+begin_src R
    quantile(xdata, prob=1.5)
  #+end_src
- ~quantile~ supports *nine* different algorithms. The ~help(quantile)~
  reveals that different statistical programming languages (S, SPSS,
  SAS) use different algorithms.
* Quantiles and summary with functions

- ~quantile(x,prob=c(0,0.25,0.5,0.75,1)~ is the 5-number summary
  consists of:
  1) the minimum (0th quantile/percentile) or minimum
  2) the 1st/lower quartile (0.25th quantile/25th percentile)
  3) the 2nd quartile or median (0.5th quantile/50th percentile)
  4) the 3rd or upper quartile (0.75th quantile/75th percentile)
  5) the 4th quartile (1st quantile/100th percentile) or maximum

- This summary is also computed by ~summary~
  #+begin_src R
    summary(xdata)
  #+end_src

  #+RESULTS:
  :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  :   2.000   2.000   2.600   2.825   3.250   4.400

* Practice: ~quantile~ and ~summary~

1) Compute the lower and upper quartiles (25th and 75th percentile or
   0.25th and 0.75th quantile) of the weights of the chicks in the
   built-in ~chickwts~ data frame.
   #+begin_src R
     quantile(chickwts$weight, prob=c(0.25,0.75))
   #+end_src

   #+RESULTS:
   :   25%   75%
   : 204.5 323.5

2) What do these results mean?
   #+begin_quote
   25% of the chicks weights lie at or below 204.5 grams, and 75% of
   the chick weights lie at or below 323.5 grams.
   #+end_quote

3) Compute the five-number summary and the sample mean of the
   magnitude of the seismic events off the coast of Fiji that occurred
   at a depth of less than 400 km, using the built-in ~quakes~ data
   frame.
   #+begin_src R
     summary(quakes$mag[quakes$depth<400])
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :    4.00    4.40    4.60    4.67    4.90    6.40

4) What do these results mean?
   #+begin_quote
   Most of the quakes below that depth of 400km lie around 4.6 on the
   Richter scale. The maximum is much further away from the upper
   quartile than the minimum is from the lower quartile. This suggests
   that the distribution of quake magnitude vs. depth is skewed. More
   specifically, it's skewed to the right - i.e. it stretches out more
   positively from the center to the right. The mean is dragged up by
   this skewedness.
   #+end_quote
   #+begin_src R :exports both :session :results output graphics file :file ../img/5_quakebox.png
     index <- quakes$depth<400
     y <- quakes$mag[index]
     boxplot(y,
             data=quakes,
             xlab="Magnitude on the Richter scale",
             main="Quakes < 400 km depth",
             horizontal=TRUE)
   #+end_src

   #+RESULTS:
   [[file:../img/5_quakebox.png]]
   #+begin_src R :exports both :session :results output graphics file :file ../img/5_quakehist.png
     hist(y,
          main="Quakes < 400 km depth",
          xlab="Magnitude on the Richter scale")
   #+end_src

   #+RESULTS:
   [[file:../img/5_quakehist.png]]
* Spread: variance, standard deviation and IQR

- Measures of centrality indicate where your observations are /massed/,
  but they say nothing about the degree of /dispersion/ or /spread/

- The measures of spread include: variance, standard deviation, and
  IQR

* Example: same centrality, different spread

- Define two vectors of hypothetical observations, ~xdata~ and ~ydata~
  #+begin_src R
    (xdata <- c(2, 4.4, 3, 3, 2, 2.2, 2, 4))
    (ydata <- c(1, 4.4, 1, 3, 2, 2.2, 2, 7))
  #+end_src

  #+RESULTS:
  : [1] 2.0 4.4 3.0 3.0 2.0 2.2 2.0 4.0
  : [1] 1.0 4.4 1.0 3.0 2.0 2.2 2.0 7.0

- These vectors have the same arithmetic mean
  #+begin_src R
    mean(xdata)
    mean(ydata)
  #+end_src

  #+RESULTS:
  : [1] 2.825
  : [1] 2.825

- Let's plot the vectors on top of one another using some base R
  plotting functions: ~plot~, ~abline~, ~text~, ~points~, and ~jitter~.

- Remember that, for plots in Org-mode, you need additional header
  arguments after the ~#+begin_src R~ - to store a graph in ~plot.png~:

  ~:results graphics file :file plot.png~

- The first code block only plots some guiding lines and labels
  #+name: DrawFrame
  #+begin_src R :results graphics file :file ../img/5_xy.png :exports both
    plot(x=xdata,    # data to plot - one vector only
         type="n",   # don't plot anything actually (n='nothing')
         xlab="",    # empty x-axis label
         ylab="data vector", # y-axis label
         yaxt="n",   # suppress y-axis
         bty="n",    # remove box around plot
         main="Comparing two data vectors with identical mean"
         )

    abline(h=c(3,3.5), # draw horizontal line at y=3 and y=3.5
           lty=2,      # draw a dashed line
           col="gray"  # draw a gray line
           )

    abline(v=2.825, # draw vertical line at x=2.825 (the mean)
           lwd=2,   # draw a thick line
           lty=3    # draw a dotted line
           )

    text(x=c(0.8,0.8),     # location of text boxes
         y=c(3,3.5),
         labels=c("x","y")
         )
  #+end_src

  #+RESULTS: DrawFrame
  [[file:../img/5_xy.png]]

- The second code block contains all of the above and plots the points
  in "jittered" fashion, because some of the vector data are
  identical:
  #+begin_src R :exports both :results graphics file :file ../img/5_xypoints.png :noweb yes
    <<DrawFrame>>

    points(                       # draw points
      jitter(c(xdata,ydata)),     # jitter the data points
      c(rep(3, length(xdata)),    # plot xdata over lower line
        rep(3.5, length(ydata))), # plot ydata over upper line
      cex=1.5                     # scale point size by 1.2
    )
  #+end_src

  #+RESULTS:
  [[file:../img/5_xypoints.png]]

- The observations in ~ydata~ (upper horizontal line) are more spread
  out around the measure of centrality (dotted vertical line) than the
  observations in ~xdata~.

- To quantify these differences, you need exact measures of spread
  like variance, standard deviation, and interquartile range (IQR)
* Variance

- The /sample variance/ measures the degree of the spread of numeric
  observations around their arithmetic mean via average squared
  distance

- Shown here for a set of n numeric measurements x = {x_1, x_2,..., x_n}
  #+attr_html: :width 500px
  [[../img/5_variance.png]]

- For example, for ~xdata~
  #+attr_html: :width 500px
  [[../img/5_variance1.png]]

- Why do we use the square to measure average distance? Because the
  average distance of values from the mean is always 0.

- The arithmetic/sample average of 1,2,9 is 4.
  #+begin_src R
    mean(c(1,2,9))
  #+end_src

  #+RESULTS:
  : [1] 4
  
- Positive and negative  distances cancel each other:
  #+attr_html: :width 700px
  [[../img/avgdist.png]]

- Why is the denominator for the variance ~n-1~ and not ~n~?
  #+begin_notes
  The use of ~n-1~ vs. ~n~ in the denominator of the formula for the
  variance and, consequentially, for the standard deviation, is the
  subject of much discussion among statisticians. It is called Bessel
  correction to correct the bias in the estimation of population
  variance. At the same time, it increases the mean squared error in
  the same estimations. With ~n-1~, you are not exactly calculating the
  average squared distance, but it is approached as the sample size ~n~
  increases. It is used when calculating spread for samples rather
  than for (ideal) populations - so in practice it is mostly used.
  #+end_notes

* Standard deviation (~sd~)

- Thinking about "average of the distance from the mean squared" is
  not very intuitive. To fix that, we can take the square root.
  
- The /standard deviation/ is the square root of the variance
  #+attr_html: :width 300px
  [[../img/5_sd.png]]

- For example, for ~xdata <- c(2, 4.4, 3, 3, 2, 2.2, 2, 4)~
  #+attr_html: :width 200px
  [[../img/5_sd1.png]]

- Roughly speaking, 0.953 represents the average distance of each
  observation from the mean. 

- [X] Is it better if the standard deviation is large or small?
  #+begin_notes
  It depends. If you manufacture machine parts, you want \sigma to be small
  so that you can be sure that all pieces are about the same. If
  you're looking at wages in a large company, I'll naturally be large,
  because there is a large spread between low and high earners.
  #+end_notes

- Interesting: the conflict between model-centric vs. data-centric data science (ref. Andrew Ng, Stanford U)
* Ideal and real spread

- For numerical data, we expect values to conform to the normal or
  Gaussian distribution described by a "bell curve".

- The plot shows a 'bell curve' of Gaussian density generated from
  random points.
  #+begin_src R :file ../img/5_gaussian.png :results graphics file :exports both
    par(mfrow=c(1,1))
    x <- seq(from=-4,to=4,by=0.02)
    y <- dnorm(x, mean = 0, sd = 1)
    plot(x,y, type="p", pch=1,
         main="The 'bell curve' of Gaussian density",
         xlab="Observed x value",
         ylab="Probability density p(x)")
    abline(v=mean(x), col="red", lty=1)
    abline(v=1, col="green", lty=2)
    abline(v=-1, col="green", lty=2)
    abline(v=2, col="gold", lty=2)
    abline(v=-2, col="gold", lty=2)
    abline(v=3, col="blue", lty=2)
    abline(v=-3, col="blue", lty=2)
  #+end_src

  #+RESULTS:
  [[file:../img/5_gaussian.png]]

- One use of this distribution: when comparing standard deviation and
  mean across different data sets, we have to transform each set of
  data into a more generic distribution with a mean of 0 and a
  standard deviation of 1.

- Base R has many different tools to characterize such data, e.g.
  1) Scatterplot
  2) Histogram of random numbers
  3) Density estimate
  4) Normal QQ-plot

- Here is how this looks like for real data, the ~geyser~ data set of
  299 eruptions of the Old Faithful geyser in Yellowstone National
  Park, WY.
  #+begin_src R :file ../img/5_geyser.png :results graphics file  :exports both
    library("car")
    library("MASS")
    par(mfrow=c(2,2))
    x <- geyser$duration
    plot(x, type="p", pch=1,
         main="Observed data values",
         xlab="Observation number",
         ylab="Observed value")
    hist(x,
         main="Histogram",
         xlab="Range of observed values",
         ylab="Number of cases in interval")
    plot(density(x),
         main="Density estimate")
    qqPlot(x, col="red",
           main="Normal QQ-plot",
           xlab="Norm quantiles",
           ylab="Sorted data value")
  #+end_src

  #+RESULTS:
  [[file:../img/5_geyser.png]]

* Interquartile Range (~IQR~)

- The interquartile range (IQR) is not computed with respect to the
  sample mean

- It measures the "middle 50%" of the data - the range of values that
  lie within a 25% quartile on either side of the median

- It is computed as the difference between the upper and lower
  quartiles of the data. For example for ~xdata~ these values were:
  #+begin_src R
    xdata <- c(2, 4.4, 3, 3, 2, 2.2, 2, 4)
    quantile(xdata, prob=c(0.25,0.75))
  #+end_src

  #+RESULTS:
  :  25%  75% 
  : 2.00 3.25

- If Q_x() denotes the quartile function, then IQR_x =
  Q_x(0.75)-Q_x(0.25), and IQR_xdata=3.25-2.00=1.25

* R functions

- Let's compute spread stats in R:
  #+begin_src R
    var(xdata)   # variance
    sd(xdata)    # standard deviation
    IQR(xdata)   # interquartile range
  #+end_src
- Confirm the definitory relationship between variance and standard
  deviation numerically:
  #+begin_src R
    identical(sqrt(var(xdata)),sd(xdata))
  #+end_src
- Let's confirm the definition of the IQR numerically:
  #+begin_src R
    as.numeric(quantile(xdata,0.75)-quantile(xdata,0.25))
  #+end_src
- Note that ~as.numeric~ strips away the percentile annotations of
  ~quartile~ results

- Compute standard variation and IQR for ~ydata~:
  #+begin_src R
    sd(ydata)
    IQR(ydata)
  #+end_src

- Confirm that ~ydata~ are more spread out than ~xdata~
  #+begin_src R
    paste("Are ydata more spread out than xdata?")
    paste("Standard deviation: ", sd(ydata) > sd(xdata))
    paste("Interquartile range: ",IQR(ydata) > IQR(xdata))
  #+end_src

* Practice: chick weights and quakes

- Let's return to chicks and quakes

- We computed the mean ~weight~ of all chicks in the ~chickwts~ data set
  #+begin_src R
    weights <- chickwts$weight
    mean(weights)
  #+end_src

- How far is the weight of each chick on average away from the mean?
  #+begin_src R
    sd(weights)
  #+end_src

- Technically, this value is the square root of a function of the
  squared distances of all observations in the sample

- We computed the five-point summary of the magnitudes of some of the
  earthquakes in the ~quakes~ data set
  #+begin_src R
    magnitudes <- quakes$mag[quakes$depth < 400]
    summary(magnitudes)
  #+end_src

- What is the width, in units of the Richter scale, of the middle 50%
  of these observations?
  #+begin_src R
    IQR(magnitudes)
  #+end_src

* IN PROGRESS Glossary: concepts

#+name: tab:terms
| TERM                    | MEANING                                     |
|-------------------------+---------------------------------------------|
| Quantile                | Subset of sample                            |
| Percentile              | Quantile in percent                         |
| Upper quartile          | 0.25th quantile/25th percentile             |
| Lower quartile          | 0.75th quantile/25th percentile             |
| Tukey's 5-point summary | Min/Max/Median/Lower & upper quartile       |
| Variance                | Average distance from sample mean squared   |
| Standard deviation      | Average distance from the sample mean       |
| Interquartile range     | Difference between Upper and Lower quartile |

* IN PROGRESS Glossary: code

#+name: tab:code
| CODE     | MEANING                   |
|----------+---------------------------|
| ~quantile~ | Quantile (parameter ~prob~) |
| ~summary~  | Five-point summary & mean |
| ~var~      | Variance                  |
| ~sd~       | Standard deviation        |
| ~IQR~    | Interquartile range       |

* References

- [[https://nostarch.com/bookofr][Davies TD (2016). Book of R. NoStarch Press. URL: nostarch.com]]
- [[https://www.seismosoc.org/inside/eastern-section/jesuit-contribution-seismology/][Udias A (1996). The Jesuit Contribution to Seismology. URL: seismosoc.org]]
