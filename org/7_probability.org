#+TITLE: Probability
#+AUTHOR: MARCUS BIRKENKRAHE
#+SUBTITLE: Applied math for data science (DSC 482/MTH 445) Fall 2022
#+PROPERTY: header-args:R :session *R* :results output :exports both
#+STARTUP: overview hideblocks indent inlineimages entitiespretty
#+attr_html: :width 300px
#+caption: Did the Sun just explode? (It's night, so we're not sure) by xkcd
[[../img/7_xkcd.png]]
Source: [[https://www.explainxkcd.com/wiki/index.php/1132:_Frequentists_vs._Bayesians][xkcd cartoon 1132: Frequentists vs. Bayesians]]

- Frequentist vs. Bayesian Probability
- Intersection, Union and Complement
- Monte Carlo Simulations
- Simulating a deck of playing cars
- The ~gtools~ R combinatorics and utilities package
- ~sample~, ~replicate~, ~expand.grid~, ~combn~, ~combinations~, ~permutations~

* What is a 'probability'?
[[../img/7_probability_and_statistics.png]]

- A number that describes the magnitude of chance associated with
  making a particular observation or statement.

- Probability is always a continuous number ~p \in [0,1]~.

- This abstract-mathematical notion becomes tangible only in context
  with an (observable) event that can be predicted

- Probability theory predicts event data based on models - e.g. "each
  /outcome/ on a fair die has a probability of 1/6 in any given roll"

- Statistics predicts models given event data, e.g. "the /average/ of a
  series of ~n~ observations is the sum of their values over ~n~."

* Events and probability

- An event A refers to a specific outcome that can occur.

- The chance that A occurs is denoted as ~Pr(A)~. ~Pr(A) = 1~ means that
  an event will occur with absolute certainty, ~Pr(A) = 0~ means that it
  cannot occur ever.

- ~Pr(A)~ is known as a /frequentist/ or /classical/ probability. It is the
  relative frequency with which an event occurs over many identical,
  objective trials.

- /Chance/ computations are /always/ models that live and die on their
  assumptions, i.e. reality is modeled as an abstraction.

- Remember: real life is not an abstraction but a mess of details:
  #+attr_html: :width 600px
  [[../img/7_probability.png]]
  [[https://www.explainxkcd.com/wiki/index.php/881:_Probability][Source: xkcd explained - cancer comics]][fn:2]

* Example 1: fair die
#+attr_html: :width 400px
[[../img/7_fairdie.png]]

  - Example: the six-sided, "fair" die, which does not exist, has
    ~Pr(A) = 1/6~ for every one of its outcomes ~A \in {1,2,3,4,5,6}~. The
    die roll is assumed to be /identical/ and /objective/, though it never
    really is.

  - The (empirical) law of large numbers guarantees that a large enough
    number of observations will yield the ideal ~Pr(A)~.

  - We can *simulate* this using R:
    1) define a standard, six-sided, fair die
    2) roll die once (with replacement)
    #+begin_src R
      die <- rep(x=1:6)  # define a standard die
      sample(
        x = die, # vector to sample from
        size = 1)  # size of sample - "roll die once"
    #+end_src

    #+RESULTS:
    : [1] 2

  - ~sample~ takes a sample of the specified ~size~ from the elements of ~x~
    either with or without replacement (~replace~).

* Visualization - "Monte Carlo Simulation"

- We want to repeat the die roll an infinite number of times

- Instead we repeat the experiment a large enough number of times ~N~ to
  make the results /practically equivalent/ to repeating forever

- We use the ~replicate~ function (from the ~apply~ family)
  1) define number of repetitions ~N~
  2) define event vector ~event~
  3) print frequencies of each side with ~table~
  4) print proportions with ~prop.table~
  5) print sample average with ~mean~
  6) plot the distribution using ~barplot~ (show in separate window)
  #+name: roll_die
  #+begin_src R
    die <- rep(1:6)  # define standard die
    N <- 100                                   #1
    events <- replicate(N, sample(die,1))     #2
    table(events) -> tbl                      #3
    tbl
    prop.table(table(events)) -> proportions  #4
    proportions
    mean(proportions)                         #5
    barplot(height=proportions)
  #+end_src

  #+RESULTS: roll_die
  : events
  :  1  2  3  4  5  6 
  : 15 24 11 15 14 21
  : events
  :    1    2    3    4    5    6 
  : 0.15 0.24 0.11 0.15 0.14 0.21
  : [1] 0.1666667

- The law of large number is visible in the barplot:
  #+begin_src R :results graphics file :file ../img/dieroll.png :noweb yes
    <<roll_die>>
    barplot(height=proportions)
  #+end_src

  #+RESULTS:
  [[file:../img/dieroll.png]]

* Example 2: quantum particles
#+attr_html: :width 400px
[[../img/7_cat.png]]

- In quantum physics, particles no longer have a definite position but
  instead a (non-standard) probability distribution ("Heisenberg
  uncertainty principle"[fn:1])

- Quantum physics is also a model-based abstraction of the real
  world. Its applications (like the laser, nuclear power etc.) are
  still real.

- In particle physics experiments, measuring the outcome of big data
  events relies on probabilistic simulations like Monte Carlo, which
  are also common place in risk analysis and investment data science.

* Example 3: Bayesian marriage
#+attr_html: :width 400px
[[../img/7_marriage.jpg]]

- Say you're married and arrive home much later than usual.

- Let B be the event "your partner is angry" because you're late.

- B cannot easily /objectively/ observed or computed.

- Instead, you might assign a value to Pr(B) based on experience: "I
  think Pr(B) = 0.5" because your experience tells you that your
  chances are 50-50.

- Instead of an impartial experiment, your chance computation is based
  on personal impression and knowledge of your spouse or mood, and it
  is not easily /reproducable/.

- This is known as /Bayesian/ probability, which uses prior knowledge or
  subjective belief to inform the computation (smaller samples needed)

- By contrast, we're looking at the /frequentist/ or classical
  interpretation of probability, which implies "objectivity"

#+attr_html: :width 300px
#+caption: Did the Sun just explode? (It's night, so we're not sure) by xkcd
[[../img/7_xkcd.png]]
Source: [[https://www.explainxkcd.com/wiki/index.php/1132:_Frequentists_vs._Bayesians][xkcd cartoon 1132: Frequentists vs. Bayesians]]

* Conditional probability

/Die example:/
#+begin_quote
Event A: "you roll a 4 or more" - Pr(A) = Pr({4,5,6}) = 3/6 = 1/2
Event B: "you roll an even number" - Pr(B) = Pr({2,4,6}) = 3/6 = 1/2
#+end_quote

- A /conditional/ probability is the probability of one event occurring
  after taking into account the occurrence of another event.

- ~P(A|B)~ is the probability that A occurs /given/ that B has occurred.
- If ~Pr(A|B) = Pr(A)~ then A and B are (stochastically) /independent/
- If ~Pr(A|B) \ne Pr(A)~ then A and B are (stochastically) /dependent/
- Generally, ~Pr(A|B) \ne Pr(B|A)~

/Die example:/
- If B has occurred already, an even number {2,4,6} has been rolled,
  and the chance to roll a 4 or more is ~Pr(A|B) = 2/3~ ({4} or {6} out
  of {2,4,6})

- Consequently, A and B are dependent: "the chance to roll a 4 or more
  is greater if an even number has already been rolled"[fn:3]

* Intersection
#+attr_html: :width 300px
[[../img/7_intersection.png]]

- The intersection of two events A and B, ~Pr(A \cap B)~ is the
  probability that both A and B occur simultaneously

- ~Pr(A \cap B) = Pr(A|B) \times Pr(B)~ or ~Pr(B|A) \times Pr(A)~

- ~Pr(A \cap B) = 0~ means A and B are /mutually exclusive/ and cannot occur
  simultaneously

- If A and B are independent, ~Pr(A|B) = Pr(A)~ or ~Pr(B|A) = Pr(B)~ and
  ~Pr(A \cap B) = Pr(A) \times Pr(B)~

- Die example: what's the probability that on a single toss you roll
  an even number (B) /and/ it's a 4 or more (A)?

- Since ~Pr(A|B) = 2/3~ and ~Pr(B) = 1/2~, ~Pr(A \cap B) = 2/3 \times 1/2 = 1/3~

- A and B are /not/ mutually exclusive since ~Pr(A \cap B) \ne 0~: it's
  possible to roll a number that's both even and at least 4.

- In R, the ~intersect~ function performs set intersection of two
  vectors:
  #+begin_src R
    (x <- c(sort(sample(1:20, 7)), NA))
    (y <- c(sort(sample(3:23, 6)), NA))
    intersect(x,y)
  #+end_src

  #+RESULTS:
  : [1]  2  3  4  6  8  9 14 NA
  : [1]  3  4 11 12 13 17 NA
  : [1]  3  4 NA

- Does ~intersect~ also work on strings?
  #+begin_src R
    DE <- c("Dr.", "Marcus", "Speh", "Birkenkrahe")
    US <- c("Dr.", "Alice", "Jewel", "Speh")
    intersect(DE,US)
  #+end_src

* Union
#+attr_html: :width 300px
[[../img/7_union.png]]

- ~Pr(A \cup B)~ is the probability that either A or B occurs.

- ~Pr(A \cup B) = Pr(A) + Pr(B) - Pr(A \cap B)~

- The intersection needs to be subtracted not to count it twice

- If A and B are mutually exclusive then ~Pr(A \cup B) = Pr(A) + Pr(B)~

- /Die example/: the probability that you observe an even number or one
  that is at least 4 is ~Pr(A \cup B) = 1/2 + 1/2 - 1/3 = 4/6 = 2/3~
  #+begin_src R
    all.equal(1/2+1/2-1/3, 2/3)
  #+end_src

- In R, the ~union~ function performs set union of two vectors:
  #+begin_src R
    (x <- c(sort(sample(1:20, 7)), NA))
    (y <- c(sort(sample(3:23, 6)), NA))
    union(x,y)
  #+end_src

  #+RESULTS:
  : [1]  1  6  7  8  9 12 14 NA
  : [1]  4  5 14 18 19 23 NA
  :  [1]  1  6  7  8  9 12 14 NA  4  5 18 19 23

* Complement
#+attr_html: :width 300px
[[../img/7_complement.png]]

- The complement of ~Pr(A)~ is the probability that A does not occur
  (usually written with an overline), or ~1 - Pr(A)~

- /Die example/: the probability that you do not roll a 4 or greater (A)
  is ~1 - 1/2 = 1/2~. If none of {4,5,6} is obtained, you must have
  rolled a {1,2,3}.

* IN PROGRESS Practice: probability with a deck of cards
* IN PROGRESS Bayes' theorem
#+attr_html: :width 200px
#+caption: Thomas Bayes, statistician and Presbyterian minister (1701-1761)
[[../img/7_Thomas_Bayes.gif]]

* IN PROGRESS Computational probability

- The common mathematical approach to probability is a /sample space/,
  the space of all possible events. For example, for two dice rolls:
  #+attr_html: :width 300px
  [[../img/7_samplespace.png]]

- In a theoretical treatment (independent dice rolls), we place
  weights of 1/36 = 1/6 x 1/6 on each of the points in the sample
  space.

- Let X and Y denote the number of dots we get on two dice, one /blue/
  and one /yellow/, and consider the meaning of Pr(X + Y = 6),
  i.e. rolling a total of 6 with two dice rolls.

- The possible outcomes with Pr(X+Y=6) are: (1,5), (2,4), (3,3),
  (4,2), (5,1), i.e. Pr(X+Y=6) = 5/36.

- Unfortunately, the notion of sample space becomes mathematically
  very tricky for more complex models, requiring /measure theory/ (Tao,
  2011), and one looses all intuition.

  | NOTEBOOK LINE | OUTCOME | BLUE + YELLOW = 6? |
  |---------------+---------+--------------------|
  |               |         |                    |

* TODO Practice: set theory operators
* References

- [[https://nostarch.com/bookofr][Davies TD (2016). Book of R. NoStarch Press. URL: nostarch.com]]

- Matloff N (2019). Probability and Statistics for Data Science. CRC
  Press.

- [[https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf][Tao T (2011). An introduction to measure theory. Am Math Soc.]]

* Footnotes

[fn:3]In this example, ~Pr(A|B) = Pr(B|A)~ - if A has occurred already,
one of {4,5,6} has been rolled, and the chance to roll an even number
is also ~Pr(B|A) = 2/3~ ({4} or {6} out of {4,5,6}).

[fn:2]Apparently, Randall Munroe's, the author of the xkcd cartoon's
fianceÃ© had cancer and passed away a few days after this comic was
posted. Its subtitle is: "My normal approach is useless here, too".

[fn:1]One of these paradoxes is the [[https://en.wikipedia.org/wiki/Uncertainty_principle][Heisenberg uncertainty principle]]:
"We cannot know both the position and the speed of a particle, such as
a photon or electron, with perfect accuracy": \Delta x \Delta y \sim h
