#+TITLE: COVARIANCE, CORRELATION AND OUTLIERS
#+AUTHOR: MARCUS BIRKENKRAHE
#+SUBTITLE: Applied math for data science (DSC 482/MTH 445) Fall 2022
#+PROPERTY: header-args:R :results output :exports both :session *R*
#+STARTUP: overview hideblocks indent inlineimages entitiespretty
#+attr_html: :width 700px
#+caption: Photo from one of Milgram's experiments 1961-63, Yale U.
[[../img/milgram.jpg]]

1. Covariance
2. Correlation
3. Outliers

* Measures of joint variability

- To identify trends, you can assess the /relationship/ between two
  numeric variables: do they increase or decrease together, and how?

- Two linked measures are used to express this quality: covariance and
  correlation, quantifying degree and direction of joint change

- Their link is comparable to variance vs. standard deviation in the
  sense that the correlation coefficient is the more commonly used
  measure to (quickly) assess the relationship

* Example: mileage and weight in ~mtcars~

- Example: mileage (~mpg~) and weight (~wt~) in ~mtcars~
  #+name: plot
  #+begin_src R :results graphics file :file ../img/6_mtcars.png
    plot(mtcars$mpg ~ mtcars$wt,
         xlab="weight",
         ylab="mileage",
         main="Mileage vs. weight in mtcars")
  #+end_src

  #+RESULTS: plot
  [[file:../img/6_mtcars.png]]

- We can easily fit a linear model ~lm~ through the sample:
  #+begin_src R :results graphics file :file ../img/6_mtcars1.png :noweb yes
    <<plot>>
    abline(lm(mpg~wt,data=mtcars),      # the model
           lty=3,lwd=2,col="steelblue") # layout details
    legend(x="topright",                # plot legend
           legend="linear model",
           lty=3,lwd=2,col="steelblue")
  #+end_src

  #+RESULTS:
  [[file:../img/6_mtcars1.png]]

- We can read some qualities of the joint change straight from the
  plot or the associated numbers:
  #+begin_src R
    lm(mpg~wt,data=mtcars)
  #+end_src

  #+RESULTS:
  : 
  : Call:
  : lm(formula = mpg ~ wt, data = mtcars)
  : 
  : Coefficients:
  : (Intercept)           wt  
  :      37.285       -5.344

* Covariance

- The covariance expresses how much two variables change together and
  the nature of this change (positive or negative)

- Positive change means that both variables increase together

- Negative change means that both variables decrease together

- Covariance for a sample of ~n~ observations for two variables ~x~ and ~y~
  in relation to the respective sample mean values[fn:2]:
  #+attr_html: :width 300px
  [[../img/6_covariance.png]]

- Positive r_{xy} indicates a positive relationship

- Negative r_{xy} indicates a negative relationship

- r_{xy}= 0 indicates that there is no linear relationship

- Also, not that r_{xy} \equiv r_{yx} i.e. the order of variables is irrelevant

- The variance is a special case of the covariance, in which the two
  variables are identical - i.e. it measures "covariance with itself"

- What is the unit of measurement of the covariance?
  #+begin_quote
  If ~x~ was measured in u_x, and ~y~ in u_y, then the unit of measurement
  of the bivariate covariance of these variables would be u_x* u_y -
  e.g. if we measure the joint variability of dollars and years, the
  covariance is measured in "dollar-years".
  #+end_quote

* Example

- Let's go back to ~xdata~ and ~ydata~ considered before to illustrate
  measures of spread:
  #+begin_src R
    xdata <- c(2, 4.4, 3, 3, 2, 2.2, 2, 4)
    ydata <- c(1, 4.4, 1, 3, 2, 2.2, 2, 7)
    sd(xdata)  # small spread
    sd(ydata)  # large spread
    mean(xdata-ydata) # identical mean
  #+end_src

  #+RESULTS:
  : [1] 0.9528154
  : [1] 2.012639
  : [1] 0

- Computing the sample covariance (~digits=4~):
  #+attr_html: :width 400px
  [[../img/6_covariance1.png]]

- [ ] Compute this using R "by hand":
  #+begin_src R
    m <- mean(xdata)
    ((2-m)*(1-m)+
     (4.4-m)*(4.4-m)+
     (3-m)*(1-m)+
     (3-m)*(3-m)+
     (2-m)*(2-m)+
     (2.2-m)*(2.2-m)+
     (2-m)*(2-m)+
     (4-m)*(7-m))/(length(xdata)-1)
  #+end_src

  #+RESULTS:
  : [1] 1.479286

- Using the ~cov~ function:
  #+begin_src R
    options(digits=4)
    cov(xdata,ydata)
  #+end_src

  #+RESULTS:
  : [1] 1.479

- This suggests that there is a positive relationship based on the
  observations

- Plotting the vectors:
  #+begin_src R :results graphics file :file ../img/6_cov.png
    plot(ydata ~ xdata, pch=13, cex=2)
  #+end_src

  #+RESULTS:
  [[file:../img/6_cov.png]]

* Correlation

- Correlation allows you to interpret the covariance further by
  identifying both /direction/ and /strength/ of any association

- Correlation measures association well under controlled
  conditions but it does not ever measure causation[fn:1]

- The most common correlation coefficient is Pearson's product-moment
  correlation coefficient (the default in R) \rho_{xy} \in (-1,1) computed
  with the respective standard deviations s_x and s_y:
  #+attr_html: :width 200px
  [[../img/6_pearson.png]]

- When \rho_xy = -1 the relationship is perfectly negative

- The closer \rho_xy gets to 0, the weaker the relationship

- \rho_xy = 0 shows no relationship at all

- \rho_xy = +1 indicates a perfectly positive relationship

- Again, \rho_xy \equiv \rho_yx

- Computing \rho_{xdata,ydata} by hand using s_x = 0.953 and s_y = 2.013:
  #+begin_src R
    cov(xdata,ydata)/(sd(xdata)*sd(ydata))
  #+end_src

  #+RESULTS:
  : [1] 0.7714

- The result indicates a moderate to strong positive association
  between the observations in ~xdata~ and ~ydata~

- Using the ~cor~ function:
  #+begin_src R
    cor(xdata,ydata)
  #+end_src

  #+RESULTS:
  : [1] 0.7714

- [ ] Check out the ~help~ for ~cor~ or ~cov~ (same vignette), and run the
  ~example(cor)~ programs

* Checking the relationship with a linear model

- We can attempt to fit a line through the points using ~lm~
  #+begin_src R :results graphics file :file ../img/6_lmcor.png
    line <- lm(ydata ~ xdata)
    plot(ydata ~ xdata)
    abline(line, lty=3, lwd=2, col="red")
    legend(x="topleft",
           legend="perfectly linear",
           lty=3,lwd=2,col="red")
  #+end_src

  #+RESULTS:
  [[file:../img/6_lmcor.png]]

- The correlation coefficient estimates the nature of the /linear/
  relationship between these variables: points closer to a perfect
  straight line have a value \rho_xy close to either -1 or 1.

* Different values of \rho_xy

- The figure displays different scatterplots, each showing 100 points

- Observations have been generated randomly and artificially to follow
  the preset values of \rho_xy labeled above each plot
  #+attr_html: :width 600px
  [[../img/6_corexample.png]]

- The last row shows that Pearson's correlation coefficient can only
  detect linear relationships: the two last plots show distinct
  patterns but no linear correlation

* Practice: ~quakes~

*OPEN YOUR ORG-MODE PRACTICE FILE IN EMACS AND START R*

We are interested in the correlation between the number of detecting
earthquake stations and the magnitude of earthquakes detected by them.

1) *Look* at the built-in ~quakes~ data set
   #+begin_src R
     str(quakes)
     head(quakes)
     stations <- quakes$stations
     magnitudes <- quakes$mag
   #+end_src

   #+RESULTS:
   #+begin_example
   'data.frame':	1000 obs. of  5 variables:
    $ lat     : num  -20.4 -20.6 -26 -18 -20.4 ...
    $ long    : num  182 181 184 182 182 ...
    $ depth   : int  562 650 42 626 649 195 82 194 211 622 ...
    $ mag     : num  4.8 4.2 5.4 4.1 4 4 4.8 4.4 4.7 4.3 ...
    $ stations: int  41 15 43 19 11 12 43 15 35 19 ...
        lat  long depth mag stations
   1 -20.42 181.6   562 4.8       41
   2 -20.62 181.0   650 4.2       15
   3 -26.00 184.1    42 5.4       43
   4 -17.97 181.7   626 4.1       19
   5 -20.42 182.0   649 4.0       11
   6 -19.68 184.3   195 4.0       12
   #+end_example

2) *Plot* the observation ~stations~ against the earthquake magnitude ~mag~,
   label the axes using the ~xlab~ and ~ylab~ parameters, and ~title~ it.
   #+begin_src R :results graphics file :file ../img/6_stations.png
     plot(stations ~ magnitudes,
          xlab = "Earthquake magnitude (Richter scale)",
          ylab = paste("Observation station number (1-",
                       max(quakes$stations),")"))
     title("Correlation of no. of stations and Earthquake magnitude")
   #+end_src

   #+RESULTS:
   [[file:../img/6_stations.png]]

3) What *insights* do you get from this plot?
   - What does a single point tell you?
   - What do vertical point groups mean?
   - What correlations can you see?
   #+begin_notes
   - A single point corresponds to a pair of values: how many stations
     have detected an earthquake of a particular magnitude?
   - There are lot of points on top of one another: a single magnitude
     value seems to have been detected to different levels of
     precision (it's difficult to measure this exactly)
   - The plot shows a positive relationship: more stations tend to
     detect events of higher magnitude.
   #+end_notes

4) *Compute* the /covariance/ of these two features.
   #+begin_src R
     cov(stations,magnitudes)
   #+end_src

   #+RESULTS:
   : [1] 7.508

5) *Compute* Pearson's linear correlation coefficient.
   #+begin_src R
     cor(stations,magnitudes)
   #+end_src

   #+RESULTS:
   : [1] 0.8512

* Correlation and causation
#+attr_html: :width 600px
[[../img/causation.jpg]]

- The correlation measures /association/ not /causation/

- Causation is difficult to prove even in controlled situations

- If two variables are highly correlated, you only need one

- This "dimension reduction" is important in machine learning

* The Strangeness of Outliers
#+attr_html: :width 500px
[[../img/PeopleAreStrange.jpg]]

#+begin_quote
Â»People are strange
When you're a stranger
Faces look ugly
When you're alone

Women seem wicked
When you're unwanted
Streets are uneven
When you're down

When you're strange
Faces come out of the rain
When you're strange
No one remembers your name
When you're strange
When you're strange
When you're strange
People are strange
All right, yeahÂ«

[[https://youtu.be/j0Mz_IqpZX8][THE DOORS]]
#+end_quote

* Definition and example
#+attr_html: :width 500px
[[../img/outliers_quote.png]]

- An /outlier/ is an /anomalous/ observation that does not appear to "fit"
  with the bulk of the data (and that may be hard to explain)

- Outliers correspond to extreme values but there is no numeric rule
  as to what constitutes an 'extreme' event

- Observing human "outliers" (or eccentrics) also leads to a lot of
  extreme qualitative values (what do you think they are?)
  1) extreme kindness
  2) extreme work ethics (+/-)
  3) extremely talented
  4) extreme intelligence
  5) extreme hoarder
  6) extreme height
  7) extreme wealth
  8) extreme occupation
  9) extremely funny
  10) extremely consistent

- *Whom do you know who would qualify as a "human outlier"? Why?*

* Univariate example

- Ten hypothetical data points
  #+begin_src R :results silent
    foo <- c(0.6, -0.6, 0.1, -0.2, -1.0, 0.4, 0.3, -1.8, 1.1, 6.0)
  #+end_src

- Plot the points with ~plot~ - you can already see the outlier, but
  it's hard to see any clustering effects for univariate data unless
  they're printed on a line.
  #+begin_src R :results graphics file :file ../img/outlier.png
    plot(foo)
  #+end_src

  #+RESULTS:
  [[file:../img/outlier.png]]

- Plot the points on a line to see the distribution more clearly: most
  of the observations are centered around 0 but one value is out at 6.

  Save the plot in the file ~outlier1.png~.
  #+begin_src R :results graphics file :file ../img/outlier1.png
    plot(
      x = foo,        # univariate data
      y = rep(0,10),  # substitute 2nd dimension
      yaxt = "n",     # no y-axis
      ylab = "",      # no y-label
      bty = "n",      # no frame
      cex = 2,        # double point size
      cex.axis = 1.5, # increase axis and label size
      cex.lab = 1.5)

    arrows(x0=5,y0=0.5,  # arrow starting point
           x1=5.9,y1=0.1, # arrow end point
           lwd=2)  # double line width

    text(x=5,y=0.7, # location of textbox
         labels="Outlier?",
         cex=3)
  #+end_src

  #+RESULTS:
  [[file:../img/outlier1.png]]

* Bivariate example

- We define two more ten-element example vectors, ~bar~ and ~baz~:
  #+begin_src R :results silent
    bar <- c(0.1, 0.3, 1.3, 0.6, 0.2, -1.7, 0.8, 0.9, -0.8, -1.0)
    baz <- c(-0.3, 0.9, 2.8, 2.3, 1.2, -4.1, -0.4, 4.1, -2.3, -100.0)
  #+end_src

- Plot ~x = bar~ and ~y = baz~ without any customizations at first and
  save the plot in ~outlier2.png~
  #+begin_src R :results graphics file :file ../img/outlier2.png
    plot(
      x = bar,
      y = baz)
  #+end_src

  #+RESULTS:
  [[file:../img/outlier2.png]]

- Add the commands to double the point size ~cex~ and increase axis and
  label size, ~cex.axis~ and ~cex.lab~ by ~1.5~.
  #+begin_src R :results graphics file :file ../img/outlier3.png
    plot(
      x = bar,
      y = baz,
      cex = 2,
      cex.axis = 1.5,
      cex.lab = 1.5)
  #+end_src

  #+RESULTS:
  [[file:../img/outlier3.png]]

- Add an arrow that points at the outlier in the lower half of the
  plot using the function ~arrows~. Double the line width ~lwd~.
  #+begin_src R :results graphics file :file ../img/outlier4.png
    plot(
      x = bar,
      y = baz,
      cex = 2,
      cex.axis = 1.5,
      cex.lab = 1.5)

    arrows(x0=-0.5,y0=-80,
           x1=-0.95,y1=-97,
           lwd=2)
  #+end_src

  #+RESULTS:
  [[file:../img/outlier4.png]]

- Add the text ~"outlier?"~ at the start of the arrow using ~text~. Triple
  the text size ~cex~. To left-justify the textbook, add ~adj=0~.
  #+begin_src R :results graphics file :file ../img/outlier5.png
    plot(
      x = bar,
      y = baz,
      cex = 2,
      cex.axis = 1.5,
      cex.lab = 1.5)
    arrows(x0=-0.5,y0=-80,
           x1=-0.95,y1=-97,
           lwd=2)
    text(x=-0.5,y=-74,
         labels="outlier?",
         cex=3,
         adj=0)
  #+end_src

  #+RESULTS:
  [[file:../img/outlier5.png]]

* Removing outliers

- Data scientist will try to remove outliers before computing results

- Outliers can occur naturally (outlier is an accurate observation),
  or unnaturally (as the result of a contamination or false input)

- To check this, look at both plots in one image:

  #+begin_src R
    ls()  # check which variables are in the work environment
  #+end_src

  #+RESULTS:
  : [1] "bar"        "baz"        "foo"        "line"       "m"         
  : [6] "magnitudes" "stations"   "xdata"      "ydata"

  #+begin_src R :results graphics file :file ../img/outliers.png
    par(mfrow=c(1,2), pty='s')  # set up two square plots side by side

    plot(foo,rep(0,10),yaxt="n",ylab="",bty="n")
    arrows(5,0.5,5.9,0.1)
    text(5,0.7,labels="Outlier?")
    title("Plot no. 1")

    plot(bar,baz)
    arrows(-0.5,-80,-0.95,-97)
    text(-0.5,-74,labels="outlier?",adj=0)
    title("Plot no. 2")
  #+end_src

  #+RESULTS:
  [[file:../img/outliers.png]]

- Compute the sample mean and the mean once the outlier has been
  removed for ~foo~ in the left plot. Tip: use ~which~ to get the
  outlier's index.
  #+begin_src R
    mean(foo)
    outlier_foo <- which(foo==max(foo))
    mean(foo[-outlier_foo])
  #+end_src

  #+RESULTS:
  : [1] 0.49
  : [1] -0.1222

- The sample ~mean~ is greatly affected. It is not "robust" against
  outliers.

- The boxplot of both vectors shows that the 5-point-summary is little
  affected - these measures are considered "robust" against outliers
  #+begin_src R :results graphics file :file foobox.png
    boxplot(foo, foo[-outlier_foo])
  #+end_src

  #+RESULTS:
  [[file:foobox.png]]

- In the ~boxplot~, the outliers are identified as IQR * 1.5 (can be
  changed, see ~help(boxplot)~
  
- Without additional information, it is impossible to say if removing
  the outlier is sensible or not.

- Compute the correlation coeffient of ~bar~ with ~baz~ shown in the right
  plot:
  #+begin_src R
    cor(bar,baz)
    outlier_baz <- which(baz==min(baz))
    outlier_bar <- which(bar==min(bar))
    cor(bar[-outlier_bar], baz[-outlier_baz])
  #+end_src

  #+RESULTS:
  : [1] 0.4566
  : [1] -0.01537

- The correlation is much stronger without that outlier.

- Check via plot that the outliers were actually removed:
  #+begin_src R :results graphics file :file ../img/outliers_removed.png
    par(mfrow=c(1,2), pty='s')  # set up two square plots side by side

    plot(foo[-outlier_foo],rep(0,length(foo)-1),yaxt="n",ylab="",bty="n")
    arrows(5,0.5,5.9,0.1)
    title("Plot no. 1 (outlier removed)")

    plot(bar[-outlier_bar],baz[-outlier_baz])
    arrows(-0.5,-80,-0.95,-97)
    title("Plot no. 2 (outlier removed)")
  #+end_src

  #+RESULTS:
  [[file:../img/outliers_removed.png]]

* Practice: covariance, correlation, outliers

- Download the practice file from [[https://tinyurl.com/45fjmyxy][tinyurl.com/45fjmyxy]]
- Get the dataset from [[https://tinyurl.com/494vdr56][tinyurl.com/494vdr56]]
- Complete the practice file in Emacs
- Upload the completed practice file to Canvas

* Glossary: concepts

#+name: tab:terms
| TERM         | MEANING                           |
|--------------+-----------------------------------|
| Linearity    | Functions of the form y = ax + b  |
| Model        | Abstraction after removing detail |
| Fit/trend    | Aligning data with a curve        |
| Intercept    | Intercept with the y-axis         |
| Slope        | Gradient of a curve               |
| Covariance   | Measure of point variability      |
| Univariate   | Single variable                   |
| Bivariate    | Two variables                     |
| Multivariate | Multiple variables                |
| Outlier      | Anomaly or extreme value          |
| Causation    | Causal mechanism                  |
| Correlation  | cov_{xy} / sd_x* sd_y                  |

* Glossary: code

#+name: tab:code
| CODE     | MEANING                 |
|----------+-------------------------|
| ~lm~       | linear model            |
| ~formula~  | e.g. ~y ~ x~              |
| ~cov(x,y)~ | covariance of x with y  |
| ~cor(x,y)~ | correlation of x with y |
| ~cex~      | point scale             |
| ~cex.axis~ | axis label scale        |
| ~cex.lab~  | label scale             |
| ~axes~     | Draw axes or not (T/F)  |
| ~bty~      | box type (no box: ~"n"~)  |
| ~yaxt~     | y-axis type             |
| ~arrows~   | place arrow             |
| ~text~     | place textbook          |
| ~labels~   | text in ~text~            |
| ~adj~      | textbox justification   |
| ~las~      | axis label orientation  |
| ~boxplot~  | box-and-whiskers plot   |

* References

- [[https://nostarch.com/bookofr][Davies TD (2016). Book of R. NoStarch Press. URL: nostarch.com]]

* Footnotes

[fn:2]The covariance formula carries the same correction in the
denominator n-1 for samples vs. n for populations as the variance.

[fn:1]This begs the question: how can you measure causation?
